# Методы принятия решений

## Навигация

- [Метрические алгоритмы классификации](#Метрические-алгоритмы-классификации)
  - [Алгоритм ближайших соседей](#Алгоритм-ближайших-соседей)
  - [Алгоритм k взвешенных ближайших соседей](#Алгоритм-k-взвешенных-ближайших-соседей)
  - [Метод парзеновского окна](#Метод-парзеновского-окна)
  - [Сравнение алгоритмов классификации](#Сравнение-алгоритмов-классификации)
- [Баесовские алгоритмы классификации](#Баесовские-алгоритмы-классификации)
  -[Подстановочный алгоритм (Plug-in)](#Подстановочный алгоритм (Plug-in))
# Постановка задачи
 В пространстве объектов задаем функцию расстояния, характеризующая степень близости объектов. Нам дана обучающая выборка и объект, который нужно отнести к одному из существующих классов (классифицировать). Решать задачу мы будем при помощи обучающей выборки "Ирисы Фишера".
Данная выборка содержит 150 объектов-ирисов: по 50 объектов каждого из трех классов. Ирис представлен четырьмя признаками: длинной и шириной чашелистика и лепестка. 
  
# Метрические алгоритмы классификации
**Метрические методы обучения** - методы, основанные на анализе сходства объектов. Метрические алгоритмы классификации опираются на гипотезу компактности: схожим объектам соответствуют схожие ответы. 
Для поиска оптимальных параметров для каждого из рассматриваемых ниже метрических алгоритмов используется **LOO -- leave-one-out** (критерий скользящего контроля), который состоит в следующем:

1. Исключать по одному объекту  из выборки, получаем новую выборку без исключенных объектов (назовём её Xl_1).
2. Запускать алгоритм от объекта, который нужно классифицировать, на новой выборке Xl_1.
3. Завести переменную ошибки, и, когда алгоритм ошибается, Q = Q + 1 (изначально Q = 0).
4. Когда все объекты будут перебраны, вычислить LOO, частное от ошибки и количества объектов выборки.
Оптимальный алгоритм получим при минимальном скользящем контроле (LOO).
Преимущества LOO в том, что каждый объект ровно один раз участвует в контроле, а длина обучающих подвыборок лишь на единицу меньше длины полной выборки.
Недостатком LOO является большая ресурсоёмкость, так как обучаться приходится L раз.

## Алгоритм ближайших соседей
### 1NN
1. На первом шаге подбираем метрику (в данном случае это евклидово пространство). 
2. Считаем расстояние от классифицируемого объекта, до объектов выборки, заносим значения в массив расстояний. 
3. Сортируем масив по возрастанию (от ближнего элемента к дальнему). 
4. Находим класс первого элемента массива, и относим классифицируемый объект к этому классу.

Результат работы алгоритма:
  
 ![](https://github.com/Abkelyamova/SMPR_AbkelyamovaGulzara/blob/master/NN.png)
 
 ### KNN
 
Алгоритм выбирает _k_ ближайших соседей, возвращает тот класс, который среди выбранных встречается большее количество раз и относит классифицируемый объект *u* этому классу.
Для оценки близости объекта *u* к классу *y* алгоритм **kNN** использует следующую функцию: ![](http://latex.codecogs.com/svg.latex?%5Clarge%20W%28i%2C%20u%29%20%3D%20%5Bi%20%5Cleq%20k%5D) , где *i* -- порядок соседа по расстоянию к классифицируемому объекту u.
LOO для KNN показал что оптимальное k = 6.
Результат работы алгоритма: 
![](https://github.com/Abkelyamova/SMPR_AbkelyamovaGulzara/blob/master/6NN.png)

Скользящий контроль для алгоритма KNN:
```diff
LOO <- function(xl,class) 
{
  n <- dim(xl)[1];
  loo <- rep(0, n-1) 
  for(i in 1:n)
  {
    X <- xl[-i, 1:3]
    u <- xl[i, 1:2]
    orderedXl <- sortObjectByDist(X, u)
    for(k in 1:(n-1))
    {
      test <- knn(X,u,k,orderedXl)
      if(colors[test] != colors[class[i]])
      {
        loo[k] <- loo[k]+1;
      }
    }
  }
```

![](https://github.com/Abkelyamova/SMPR_AbkelyamovaGulzara/blob/master/loo_knn.png)
#### Преимущества:

- При *k*, подобранном около оптимального, алгоритм "неплохо" классифицирует.

#### Недостатки:
- Нужно хранить всю выборку.
- При *k = 1* неустойчивость к погрешностям (*выбросам* -- объектам, которые окружены объектами чужого класса), вследствие чего этот выброс классифицировался неверно и окружающие его объекты, для которого он окажется ближайшим, тоже.
- При *k = l* алгоритм наоборот чрезмерно устойчив и вырождается в константу.
- Максимальная сумма объектов в *counts* может достигаться в нескольких классах одновременно.
- Точки, расстояние между которыми одинаково, не все будут учитываться.

## Алгоритм k взвешенных ближайших соседей
### KwNN
Данный алгоритм классификации относит объект *u* к тому классу *y*, у которого максимальна сумма весов из его *k* соседей, то есть объект относится к тому классу, который набирает больший суммарный вес среди k ближайших соседей.
В данном алгоритме, помимо функции расстояния, используется весовая функция, которая оценивает степень важности при классификации заданного объекта к какому-либо классу, что и отличает его от алгоритма kNN.

Результат роботы алгоритма:
![](https://github.com/Abkelyamova/SMPR_AbkelyamovaGulzara/blob/master/KwNN.png)
![](https://github.com/Abkelyamova/SMPR_AbkelyamovaGulzara/blob/master/LOO%26Kwnn.png)
#### Недостатки:
1. Приходится хранить обучающую выборку Xl целиком, что приводит к неэффективному расходу памяти. При наличии погрешностей это может привести к понижению точности классификации вблизи границ классов.
2. Исключается настройка алгоритмов по данным (крайне "бедный" набор параметров).
3. Если суммарные веса классов оказываются одинаковыми, то алгоритм относит классифицируемый объект u к любому из классов.
#### Преимущества:
При любом k алгоритм неплохо классифицирует. 
#### Чем kwnn лучше/хуже knn?
- Шире диапазон оптимальных k.
- Лучше точность на границах.

Пример показывающий преимущество метода kwNN над kNN(k=7): 
![](https://github.com/Abkelyamova/SMPR_AbkelyamovaGulzara/blob/master/knn&kwnn.png)
## Метод парзеновского окна

Вокруг классифицируемого объекта строится окно радиуса h. Считаем расстояния от него до тех элементов, которые попали в окно. Придаем им веса (чем меньше расстояние, тем больше вес), суммируем веса по классам и записываем в массив. Определяем класс с максимальной суммой весов. И классифицируемый объект определяем к этому классу.
В отличие от алгоритма kwNN, в методе парзеновского окна в качестве функции веса используется различные ядра.  Также с помощью _скользящего контроля(LOO)_, необходимо подбирать параметр «ширина окна». Надо выбрать такое _h_, которое даст меньше всего ошибок. 

Рассмотрим результаты работы алгоритма. Чаще всего применяются 5 типов ядер:

Случай прямоугольного ядра:

![](https://latex.codecogs.com/gif.latex?R(r)=\frac{1}{2}[|r|\leq&space;1])

![](https://github.com/Abkelyamova/SMPR_AbkelyamovaGulzara/blob/master/LOO_PW(R).png)
![](https://github.com/Abkelyamova/SMPR_AbkelyamovaGulzara/blob/master/pw(pg).png)

Случай треугольного ядра:
![](https://latex.codecogs.com/gif.latex?T(r)=(1-|r|)\cdot&space;[|r|\leq&space;1])
![](https://github.com/Abkelyamova/SMPR_AbkelyamovaGulzara/blob/master/LOO_PW(t).png)
![](https://github.com/Abkelyamova/SMPR_AbkelyamovaGulzara/blob/master/PW(T).png)

Случай квартического ядра:
![](https://latex.codecogs.com/gif.latex?Q(r)=\frac{15}{16}(1-r^{2})^{2}\cdot&space;[|r|\leq&space;1])
![](https://github.com/Abkelyamova/SMPR_AbkelyamovaGulzara/blob/master/LOO_PW(Q).png)
![](https://github.com/Abkelyamova/SMPR_AbkelyamovaGulzara/blob/master/PW(Q1).png)

Случай ядра Епанечникова:
![](https://latex.codecogs.com/gif.latex?E(r)=\frac{3}{4}(1-r^{2})\cdot&space;[|r|\leq&space;1])
![](https://github.com/Abkelyamova/SMPR_AbkelyamovaGulzara/blob/master/LOO_PW(EP).png)
![](https://github.com/Abkelyamova/SMPR_AbkelyamovaGulzara/blob/master/PW(EPACH).png)

Случай гауссовского ядра, (нормальное распределение):
![](https://github.com/Abkelyamova/SMPR_AbkelyamovaGulzara/blob/master/LOO_PW(G).png)
![](https://github.com/Abkelyamova/SMPR_AbkelyamovaGulzara/blob/master/PW(G).png)

Вывод: Больше всего подходит гауссовское ядро. Оно однозначно разделило классы на всей плоскости.
В остальных случаях все точки, не попавшие в окно, отсеились (в ядрах кроме гауссовского). 

## Сравнение алгоритмов классификации:
<table>
<tr><td>Метод</td><td>параметры</td><td>величина ошибок</td><tr>
<tr><td> KNN</td><td>k=6</td><td>0.033</td><tr>
<tr><td> KWNN</td><td>k=9</td><td>0.033</td<tr>
<tr><td> Парзеновское окно
(Прямоугольное ядро)</td><td>h=0.4</td><td>0.04</td<tr>
  <tr><td> Парзеновское окно
(Треугольное ядро)</td><td>h=0.4</td><td>0.04</td<tr>
    <tr><td> Парзеновское окно
(Квартическое ядро)</td><td>h=0.4</td><td>0.04</td<tr>
<tr><td> Парзеновское окно
(Ядро Епанечникова)</td><td>h=0.4</td><td>0.04</td<tr>
  <tr><td> Парзеновское окно
(Гауссовское ядро)</td><td>h=0.1</td><td>0.04</td<tr>
 </table>


# Баесовские алгоритмы классификации
   
   Мы решаем задачу __классификации__, нам нужно по известному вектору признаков __x__, определить класс  к которому принадлежит объект __a(x)__  по правилу : __a(x) = argmax P(y|x)__, для которого максимальна вероятность класса __y__ при условии __x__. 

Это тоже самое что __a(x) = argmax P(x|y)P(y)__, где __P(y)__ - априорная вероятность класса. 

Если __P(y__) одинаковы для всех классов - мы просто выбираем класс, плотность которого больше в точке __x__.

## Подстановочный алгоритм (Plug-in)

__Plug-in__ байесовский алгоритм классификации, в котором в качестве моделей восстанавливаемых плотностей рассматривают многомерные нормальные плотности.

Восстанавливая параметры  для каждого класса и подставляя в оптимальный байесовский классификатор получаем plug-in.

Чтобы узнать плотности распределения классов, алогритм восстанавливает неизвестные параметры  по следующим формулам для каждого класса :
![](https://latex.codecogs.com/gif.latex?\hat{\mu&space;}=\tfrac{1}{m}&space;\sum_{i=1}^{m}x_{i})  
![](https://latex.codecogs.com/gif.latex?\hat{\Sigma&space;}=\frac{1}{m-1}\sum_{i=1}^{m}(x_{i}-\hat{\mu&space;})(x_{i}-\hat{\mu&space;})^{T})  
где  𝜇 ∈ ℝ - математическое ожидание (центр), а  𝛴 ∈ ℝ - ковариационная матрица (симметричная, невырожденная, положительно определённая).  
Алгоритм заключается в том, чтобы найти неизвестные параметры 𝜇 и  𝛴  для каждого класса y и подставить их в формулу оптимального байесовского классификатора. В данном алгоритме мы предполагаем, что ковариационные матрицы не равны. 

Выбирая различные матрицы ковариации и центры для генерации тестовых данных, будем получать различные виды
дискриминантной функции.

### 1.Эллипс. 

Центр первого класса (2,2) Ковариационная матрица первого класса: ![](https://latex.codecogs.com/gif.latex?\bigl(\begin{smallmatrix}&space;10&space;&&space;0\\&space;0&space;&&space;1&space;\end{smallmatrix}\bigr))

Центр второго класса (15,2) Ковариационная матрица второго класса ![](https://latex.codecogs.com/gif.latex?\bigl(\begin{smallmatrix}&space;5&space;&&space;0\\&space;0&space;&&space;10&space;\end{smallmatrix}\bigr))

Результат: 

![](https://github.com/Abkelyamova/SMPR_AbkelyamovaGulzara/blob/master/Bayes_algoritms/окружность.png)

### 2. Гипербола.

Центр первого класса (1,2) Ковариационная матрица первого класса: ![](https://latex.codecogs.com/gif.latex?\bigl(\begin{smallmatrix}&space;5&space;&&space;0\\&space;0&space;&&space;1&space;\end{smallmatrix}\bigr))

Центр второго класса (5,2) Ковариационная матрица второго класса: ![](https://latex.codecogs.com/gif.latex?\bigl(\begin{smallmatrix}&space;1&space;&&space;0\\&space;0&space;&&space;5&space;\end{smallmatrix}\bigr))

Результат: 

![](https://github.com/Abkelyamova/SMPR_AbkelyamovaGulzara/blob/master/Bayes_algoritms/гипербола.png)

### 3. Парабола.

Центр первого класса (-5,0) Ковариационная матрица первого класса:![](https://latex.codecogs.com/gif.latex?\bigl(\begin{smallmatrix}&space;10&space;&&space;0\\&space;0&space;&&space;1&space;\end{smallmatrix}\bigr))

Центр второго класса (10,0) Ковариационная матрица второго класса:![](https://latex.codecogs.com/gif.latex?\bigl(\begin{smallmatrix}&space;5&space;&&space;0\\&space;0&space;&&space;10&space;\end{smallmatrix}\bigr))

Результат:

![](https://github.com/Abkelyamova/SMPR_AbkelyamovaGulzara/blob/master/Bayes_algoritms/парабола.png)
